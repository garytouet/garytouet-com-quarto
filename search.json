[
  {
    "objectID": "link/2025/11/04/anil-dash-ai-majority-view/index.html",
    "href": "link/2025/11/04/anil-dash-ai-majority-view/index.html",
    "title": "Anil Dash on today‚Äôs discourse regarding AI",
    "section": "",
    "text": "Anil Dash (emphasis is his):\n\nMost people who actually have technical roles within the tech industry, like engineers, product managers, and others who actually make the technologies we all use, are fluent in the latest technologies like LLMs. They aren‚Äôt the big, loud billionaires that usually get treated as the spokespeople for all of tech.\nAnd what they all share is an extraordinary degree of consistency in their feelings about AI, which can be pretty succinctly summed up:\nTechnologies like LLMs have utility, but the absurd way they‚Äôve been over-hyped, the fact they‚Äôre being forced on everyone, and the insistence on ignoring the many valid critiques about them make it very difficult to focus on legitimate uses where they might add value.\n\nDefinitely agree with him. It was refreshing to read.\nThese days, I spend more time than usual on LinkedIn and I follow the Tech news more consistently. I often question my own judgment and my own sanity while reading.\nThe amount of blind embrace for this technology in the mainstream‚Äôs discourse is astounding in my opinion. I‚Äôm looking forward to the next stage of the hype cycle, when reality sets in and expectations lower, to have more interesting discussions."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Title\nDate\n\n\n\n\nReading: 'The Data Warehouse Toolkit: The Definitive Guide to Dimensional Modeling' by Ralph Kimball and Margy Ross\nMy notes and perspective on this classic of data engineering\ndata engineeringbooks\n2026-01-29\n\n\nMy selection of posit::conf(2025) talks to watch on YouTube\nFour talks that I recommend among the 100+ videos available\nRpython\n2025-12-05\n\n\n\n\n \n\n'Python is not a great language for data science'\n\nI am used to reading Python vs. R posts, but this time I can relate to the differences mentioned.\nlinkpythonR\n2025-11-19\n\n\nAnalysing the sound of my fridge\nAn excuse to try AI assisted coding with Mistral AI's model.\npythonai assisted codingfun project\n2025-11-14\n\n\n\n\n \n\nAnil Dash on today's discourse regarding AI\n\nlinkai\n2025-11-04\n\n\nCreating the favicon for this website with `ggplot2`\nWe will use the package to create an image and export it in several sizes. It is not as straightforward as I thought.\nRwebsite\n2025-09-25\n\n\n\nNo matching items"
  },
  {
    "objectID": "post/2025/create-favicon-with-ggplot2/index.html",
    "href": "post/2025/create-favicon-with-ggplot2/index.html",
    "title": "Creating the favicon for this website with ggplot2",
    "section": "",
    "text": "Quoting from Mozilla‚Äôs documentation:\n\nA favicon (favorite icon) is a tiny icon included along with a website, which is displayed in places like the browser‚Äôs address bar, page tabs and bookmarks menu.\n[‚Ä¶]\nThey are used to improve user experience and enforce brand consistency. When a familiar icon is seen in the browser‚Äôs address bar, for example, it helps users know they are in the right place."
  },
  {
    "objectID": "post/2025/create-favicon-with-ggplot2/index.html#favicon-what-is-it",
    "href": "post/2025/create-favicon-with-ggplot2/index.html#favicon-what-is-it",
    "title": "Creating the favicon for this website with ggplot2",
    "section": "",
    "text": "Quoting from Mozilla‚Äôs documentation:\n\nA favicon (favorite icon) is a tiny icon included along with a website, which is displayed in places like the browser‚Äôs address bar, page tabs and bookmarks menu.\n[‚Ä¶]\nThey are used to improve user experience and enforce brand consistency. When a familiar icon is seen in the browser‚Äôs address bar, for example, it helps users know they are in the right place."
  },
  {
    "objectID": "post/2025/create-favicon-with-ggplot2/index.html#what-kind-of-graphic-am-i-trying-to-create",
    "href": "post/2025/create-favicon-with-ggplot2/index.html#what-kind-of-graphic-am-i-trying-to-create",
    "title": "Creating the favicon for this website with ggplot2",
    "section": "What kind of graphic am I trying to create?",
    "text": "What kind of graphic am I trying to create?\nI‚Äôm trying to re-create the favicon used for a previous iteration of my personal website:\n\nSquare shape\nSolid blue background color\nMy initials ‚ÄúGT‚Äù positioned on top, towards the bottom of the square.\n\nI‚Äôm starting simple. Should be doable, right?"
  },
  {
    "objectID": "post/2025/create-favicon-with-ggplot2/index.html#setup",
    "href": "post/2025/create-favicon-with-ggplot2/index.html#setup",
    "title": "Creating the favicon for this website with ggplot2",
    "section": "Setup",
    "text": "Setup\nInstalling and loading ggplot2.\n\ninstall.packages('ggplot2')\nlibrary(\"ggplot2\")\n\nHere, I set the default size of my plots for the rest of the document. It is an important setting since I will generate a square icon and, by default, dimensions are 7 x 5 inches.\n\nknitr::opts_chunk$set(\n  fig.width = 5,\n  fig.height = 5\n)"
  },
  {
    "objectID": "post/2025/create-favicon-with-ggplot2/index.html#creating-the-graphic",
    "href": "post/2025/create-favicon-with-ggplot2/index.html#creating-the-graphic",
    "title": "Creating the favicon for this website with ggplot2",
    "section": "Creating the graphic",
    "text": "Creating the graphic\nI decided to tackle the blue background first. I remembered that ggplot has different geoms to create a square. I looked up the documentation and started coding.\nI iterated through geom_raster(), geom_rect() and landed on geom_tile(). It is best suited for my use case. I just need to define the size of the square and use the origin (0,0) as the center of the square.\n\nsquare_size &lt;- 5\nmy_blue &lt;- \"#0063dc\"\n\n(p &lt;- ggplot() +\n  geom_tile(\n    aes(x = 0, y = 0, width = square_size, height = square_size),\n    fill = my_blue\n  ))\n\n\n\n\n\n\n\n\nNow, I will 1) add the text, position it, and then 2) remove all plot elements like grid lines, axes, etc.\n\n(p &lt;- p +\n  geom_text(\n    aes(\n      x = 0,\n      y = -square_size / 5,\n      label = \"GT\",\n      family = \"Inter\",\n      fontface = \"bold\"\n    ),\n    color = \"white\",\n    size = square_size / 2,\n    size.unit = 'in'\n  ) +\n  theme_void()\n)\n\n\n\n\n\n\n\n\nVoil√†!\nOn purpose, I defined the position of the text and its size as a factor of the variable square_size. I hope that it will make it easier to export this graphic in different sizes after.\nRegarding the font, I‚Äôm using Inter. I had to download and install it on Mac OS for it to become available in the plot."
  },
  {
    "objectID": "post/2025/create-favicon-with-ggplot2/index.html#saving-in-various-sizes-and-formats",
    "href": "post/2025/create-favicon-with-ggplot2/index.html#saving-in-various-sizes-and-formats",
    "title": "Creating the favicon for this website with ggplot2",
    "section": "Saving in various sizes and formats",
    "text": "Saving in various sizes and formats\nThis is when things get more complicated for two reasons:\n\nNo clear spec for favicons exists. It depends on browsers, mobile OSes. Different formats, different sizes.\nResizing a plot is not as straightforward as I thought\n\n\nFavicon specs\nWhile browsing, I found various specs1 for this favicon. I will spare you the details. In the end, I landed on the following:\n\nfavicon.ico 32x32 (px)\nfavicon.png 96x96 (px)\napple-touch-icon.png 180x180 (px)\nfavicon.svg\n\n\n\nScaling issues\nSo far, I‚Äôve set all my measurements in inches (figure size, font size). Now, my goal is to provide a size in pixels and to automatically resize and scale the icon accordingly.\nThe problem is‚Ä¶\n\nggsave(\n  p,\n  filename = \"icons/favicon_problem.png\",\n  dpi = 'retina', #retina = 320, according to documentation\n  width = 96,\n  height = 96,\n  units = 'px'\n)\n\n\n‚Ä¶ that the dimensions are good, but the proportions are not. With ggsave I can easily define the size in pixels, but this doesn‚Äôt change the font size in the plot.\nChristophe Nicault‚Äôs article Understanding text size and resolution in ggplot2 explains very well what happens. They recommend to use a package called ragg and the scaling option.\nI tried, but simply installing ragg created a big error2 in Quarto and completely destroyed my ability to preview my post. I also played around with the scale option in ggsave(), to no avail.\nAs a result, I decided to handle the scaling myself.\n\n\nDIY scaling\nSince I‚Äôve defined sizes in inches when creating the graphic and since I want to express icon dimensions in pixels, I need to understand the relationship between inches and pixels first.\nFrom Understanding text size and resolution in ggplot2:\n\nThe relation is : (size in inches) = (screen size in pixel) / PPI or (screen size in pixel) = DPI * (size in inches)\n\nIn other words, if I want an icon with size 32 x 32 pixels, the conversion in inches will be 32 / 320 = 0.1 (with a resolution of 320). I can create a function that takes these two parameters as arguments, converts that to a measurement in inches and then passes that to ggplot2.\n\n# Function that creates the icon with sizes in inches\ncreate_icon &lt;- function(square_size_inches) {\n  p &lt;- ggplot() +\n    geom_tile(\n      aes(\n        x = 0,\n        y = 0,\n        width = square_size_inches,\n        height = square_size_inches\n      ),\n      fill = my_blue\n    ) +\n    geom_text(\n      aes(\n        x = 0,\n        y = -square_size_inches / 5,\n        label = \"GT\",\n        family = \"Inter\",\n        fontface = \"bold\"\n      ),\n      color = \"white\",\n      size = square_size_inches /2,\n      size.unit = 'in'\n    ) +\n    theme_void()\n\n    return(p)\n  }\n\n# Function that uses a size in pixels and a dpi to convert it into inches, create the icon and save it to disk\ncreate_and_save_icon &lt;- function(filename, size_pixels, dpi = 320) {\n  square_size_inches &lt;- size_pixels / dpi\n  icon &lt;- create_icon(square_size_inches)\n\n  ggsave(\n    plot = icon,\n    filename = filename,\n    dpi = dpi,\n    width = size_pixels,\n    height = size_pixels,\n    units = 'px',\n    bg = my_blue #otherwise white outline in png\n  )\n}\n\nLet‚Äôs try it out!\n\ncreate_and_save_icon(filename = 'icons/favicon_32x32.png', size_pixels = 32, dpi = 320)\n\n\nIt works! I can now generate the different sizes that I need.\n\nsizes = c(32, 96, 180)\n\nfor (size in sizes) {\n  filename = paste0('icons/favicon_', size, 'x', size, '.png')\n\n  create_and_save_icon(filename = filename, size_pixels = size, dpi = 320)\n}\n\n\n\n\n\n\n\nfavicon_32x32.png\n\n\n\n\n\n\n\nfavicon_96x96.png\n\n\n\n\n\n\n\nfavicon_180x180.png\n\n\n\n\n\nAnd for the last one, the .svg version. Here, we‚Äôll use the base R built-in SVG graphics device:\n\nicon &lt;- create_icon(1)\nsvg(\"icons/favicon.svg\", width = 1, height = 1)\nprint(icon)\ninvisible(dev.off())\n\n\n\n\nfavicon.svg"
  },
  {
    "objectID": "post/2025/create-favicon-with-ggplot2/index.html#footnotes",
    "href": "post/2025/create-favicon-with-ggplot2/index.html#footnotes",
    "title": "Creating the favicon for this website with ggplot2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMost complete article that I could find: RealFaviconGenerator‚Äôs Understanding favicon elements.‚Ü©Ô∏é\nquarto preview would throw a segfault error that I was not able to resolve.‚Ü©Ô∏é"
  },
  {
    "objectID": "post/2026/data-warehouse-toolkit-definitive-guide-to-dimensional-modelling/index.html",
    "href": "post/2026/data-warehouse-toolkit-definitive-guide-to-dimensional-modelling/index.html",
    "title": "Reading: ‚ÄòThe Data Warehouse Toolkit: The Definitive Guide to Dimensional Modeling‚Äô by Ralph Kimball and Margy Ross",
    "section": "",
    "text": "A Lead Data Engineer recommended this book while we collaborated on a business intelligence project. Our product depended on a data warehouse built by our team. As his Product Manager counterpart, we frequently discussed changes affecting the warehouse. He often cited best practices from this book, a standard in the industry.\nWhile I could discuss the pros and cons of data modeling choices with him, I lacked the vocabulary and deeper conceptual knowledge to match his expertise. Throughout my career, I have worked extensively with data warehouses and handled light data engineering projects myself, but I never built a full dimensional model.\nUnderstanding these concepts and techniques seemed valuable. Online, this book is often called the guide to data modelling and data warehousing, a classic for all data engineers. Naturally curious, I decided to give it a go.\nI read the book cover to cover and didn‚Äôt skip any section. At 23 hours (per my Kindle), it is a long read. But, it‚Äôs also dense and practical.\nIn the following sections, I will highlight what stuck with me (the ideas I intend to remember) and reflect on how they connect to my own work experience. Several parts of the book brought back memories from past companies."
  },
  {
    "objectID": "post/2026/data-warehouse-toolkit-definitive-guide-to-dimensional-modelling/index.html#most-salient-sectionsconcepts-for-me",
    "href": "post/2026/data-warehouse-toolkit-definitive-guide-to-dimensional-modelling/index.html#most-salient-sectionsconcepts-for-me",
    "title": "Reading: ‚ÄòThe Data Warehouse Toolkit: The Definitive Guide to Dimensional Modeling‚Äô by Ralph Kimball and Margy Ross",
    "section": "Most salient sections/concepts for me",
    "text": "Most salient sections/concepts for me\n\nThe four key decisions when designing a dimensional model\n1. Select the business process\nThe advice is: start by modeling a business process. KPIs, metrics, reports can then be built on this fondation. Do not design your dimensional model based on the KPIs and reports available.\nExcerpt from the book:\n\nBusiness processes are the operational activities performed by your organization, such as taking an order, processing an insurance claim, registering students for a class, or snapshotting every account each month.\n\n2. Declare the grain\nThe grain defines the level of detail for the dataset i.e.¬†what a single fact table row represents.\nChoose the lowest possible grain (atomic grain) from the source systems. This approach offers the most flexibility. Users can then aggregate and manipulate the data as they wish.\n3. Identify the dimensions\nDimensions provide the context for a business process event. For example, when a sale happens, the context dimensions are date, product, store, payment method, promotion.\nIn reports, dimensions serve as descriptive attributes (typically text labels). They are also used to filter the facts and to form groups during aggregation.\n4. Identify the facts\nFacts are the numeric measurements tied to a business event. All facts for a single event are stored in one table, which includes foreign keys. These keys link each fact row to its associated dimensions (the context information).\nFrom the book:\n\nFacts are the measurements that result from a business process event and are almost always numeric. A single fact table row has a one-to-one relationship to a measurement event as described by the fact table‚Äôs grain. Thus a fact table corresponds to a physical observable event, and not to the demands of a particular report. Within a fact table, only facts consistent with the declared grain are allowed. For example, in a retail sales transaction, the quantity of a product sold and its extended price are good facts, whereas the store manager‚Äôs salary is disallowed.\n\n\n\nBusiness participation\nThe authors emphasise that strong business buy-in is essential for project success. This makes sense, but two points stood out to me. They were not obvious at first and I found them particularly valuable:\n\nStaff the project with business team members, who are also responsible for its success. Give them responsibilities and ask them to gather requirements, feedback, validate certain decisions and spread the word in the organisation.\nEstablish a data stewardship/governance program beforehand, with identifiable data stewards. As the book states:\n\nData stewardship or governance programs should focus first on the major dimensions. Depending on the industry, the list might include date, customer, product, employee, facility, provider, student, faculty, account, and so on. Thinking about the central nouns used to describe the business translates into a list of data governance efforts to be led by subject matter experts from the business community. Establishing data governance responsibilities for these nouns is the key to eventually deploying dimensions that deliver consistency and address the business‚Äôs needs for analytic filtering, grouping, and labeling. Robust dimensions translate into robust DW/BI systems.\n\n\n\n\nThe three fundamental types of fact tables\n\nTransaction fact tables: these are the most common in my experience. Each row represents a single measurement at a specific time and place. It can be an event or one line item inside an event. Rows only exist if an activity occurred. The word transaction is confusing here, because it suggests a table with sales data. In practice, they are not limited to measuring sales.\nPeriodic snapshot fact tables: these tables summarise measurement events over a fixed period (a day, week, month, ‚Ä¶). The grain is determined by the period selected. Unlike transaction tables, rows exist even if no activity took place. For example, I have used these to analyse the daily availability of inventory. A daily snapshot for each product and country was created with its availability status and its quantity.\nAccumulating snapshot fact tables: these tables represent the advancement of processes that have a clear start, standard intermediate steps and a clear end. Each row updates whenever the process reaches a new step, recording the data and relevant measurements. This structure simplifies calculating the time duration between various steps.\nIllustration from the book:\n\n\n\n\nVariations and adaptations of these fact tables:\n\nFactless fact tables\nAggregate fact tables or OLAP cubes\nConsolidated fact tables\n\n\n\nSCDs: Slowly Changing Dimensions\nDescribing these techniques succinctly is challenging. My notes below will make sense if you‚Äôve read the chapters. Otherwise, the explanations may seem abstract.\nThis section taught me the most about dimensional design, associated trade-offs and opened my mind to technical complexity. I‚Äôll admit: some SCD types took me multiple attempts to grasp. The book‚Äôs case studies were helpful for clarifying these concepts.\nThese techniques address a fundamental challenge: how to manage attribute changes in dimensions over time. For example, let‚Äôs imagine that a company updates its product categorisation system. A product previously in Sports is now in Streetwear. How should the dimension table reflect this change?\nFive core types:\n\nType 0: retain the original\n\nconsequence: facts can only be associated with the attribute‚Äôs original value\n\nType 1: overwrite\n\nconsequence: facts can only be associated with the attribute‚Äôs current value\n\nType 2: add new row\n\nintroduce a new surrogate key\nintroduce ‚Äúrow effective date‚Äù and ‚Äúrow expiration date‚Äù columns\noptional: add a ‚Äúcurrent row indicator‚Äù column\nconsequence: facts are associated with the attribute‚Äôs value in effect when the fact happened\n\nType 3: add new attribute\n\nreplace current attribute value in the target column\nadd a new column: ‚Äúprior {attribute name}‚Äù which stores the attribute value\nconsequence: facts can be associated with the current and prior attribute value\n\nType 4: add mini-dimension\n\nuseful for large dimension tables (lots of rows) where Type 2 changes are frequent (which adds many rows)\nmove frequently changing attributes to a separate dimension (seems to be frequent for customer demographics dimensions). Leave stable attributes in the original table.\ncreate one row in the mini-dimension for each combination of attribute values. Alternatively, add rows only for existing combinations and update as new ones appear.\nconsider creating value bands for continuous attributes (convert salary to salary bands, for example)\nfinally, insert a foreign key to the mini-dimension in the fact table. It captures a snapshot of the combination of attributes (for example: the customer demographics profile) at transaction time.\n\n\n3 hybrid types:\n\nType 5: mini-dimension and type 1 outrigger\n\nthis builds on Type 4 by adding a foreign key to the mini-dimension in the large dimension table (outrigger)\nexample: in a large customer dimension with a mini-dimension for demographic attributes, each time the demographic profile is updated the foreign key in the customer dimension gets updated (Type 1 change)\nthe fact table provides the demographic profile at transaction time, while the customer dimension reflects the current profile\n\nType 6: add Type 1 attributes to Type 2 dimension\n\nnote: the name is confusing. It feels like a more advanced Type 3 SCD to me\nExample: with two columns, ‚Äúhistoric department name‚Äù and ‚Äúcurrent department name‚Äù (similar to a Type 3 setup)\nwhen the department name changes:\n\napply Type 2: create a new row with ‚Äúrow effective date‚Äù, ‚Äúrow expiration date‚Äù and ‚Äúcurrent row indicator‚Äù columns with the correct dates/timestamps\napply Type 1: update the ‚Äúcurrent department name‚Äù value\n\nfacts join with the row active at transaction time, which also include the current attribute value\n\nType 7: dual Type 1 and Type 2 dimensions\n\nthe fact table includes two foreign keys:\n\none links to a dimension table managed according to Type 6\none links to a view of this dimension table showing only current values\n\nBenefit: when the fact table is queried, a choice is available. Use the historical values to filter and group by, or use the latest status\n\n\n\n\nCase study on Clickstream data\nMost of my analyst experience is in e-commerce, where Google Analytics clickstream data was the primary data source.\nI was curious to see how the authors recommended integrating this data into the data warehouse, for the following reasons:\n\nGoogle Analytics‚Äô raw data, once exported to BigQuery, is heavily nested, with a schema that takes time to master. Expecting analysts from other parts of the company or other disciplines to use is not realistic. The learning curve is too steep\nAt Zalando, clickstream data resided outside the data warehouse in a separate system (BigQuery). Joining it with the data warehouse was costly due to time-consuming data transfers.\n\nThe authors outline a solution in their case study, using these building blocks:\n\nNew dimensions: Page, Event, Session, Referrer. I‚Äôd also include User\nTwo fact tables: one at the Session grain and one at the Page grain. The Page fact table is a novel idea for me. In theory, it could track event counts per page or measurements like loading times. In practice, it would be challenging to maintain due to frequent changes in web and app experiences\nAggregate clickstream fact tables: create tables at different grains (session, month, entry page, etc.) to summarise facts\n\nFrom experience, aggregate fact tables strike me as the most practical and valuable approach. This makes for much smaller tables to query (raw clickstream data is very large). Key user behaviour facts become easily accessible. And these tables can be intelligently augmented with new attributes (for example: a flag for sessions with cart abandonment, likelihood score of being a bot).\n\n\nAudit dimensions and error event schema\nI wish I had these for debugging in the past or to investigate sudden changes in metrics.\nI saw audit dimensions in action in a previous freelance project. For example, it let us trace suspicious numbers back to the exact ingested file. It allowed us to answer questions from business stakeholders with precision. Excerpt from the book with more details:\n\nWhen a fact table row is created in the ETL back room, it is helpful to create an audit dimension containing the ETL processing metadata known at the time. A simple audit dimension row could contain one or more basic indicators of data quality, perhaps derived from examining an error event schema that records data quality violations encountered while processing the data. Other useful audit dimension attributes could include environment variables describing the versions of ETL code used to create the fact rows or the ETL process execution time stamps. These environment variables are especially useful for compliance and auditing purposes because they enable BI tools to drill down to determine which rows were created with what versions of the ETL software.\n\nError datasets:\n\nThe error event schema is a centralized dimensional schema whose purpose is to record every error event thrown by a quality screen anywhere in the ETL pipeline.\n\nI have never encountered such error tables. But, as a data warehouse user, I would appreciate this transparency.\n\n\nConformed dimensions\nFrom the book:\n\nDimension tables conform when attributes in separate dimension tables have the same column names and domain contents. Information from separate fact tables can be combined in a single report by using conformed dimension attributes that are associated with each fact table.\n[‚Ä¶]\nConformed dimensions, defined once in collaboration with the business‚Äôs data governance representatives, are reused across fact tables; they deliver both analytic consistency and reduced future development costs because the wheel is not repeatedly re-created.\n[‚Ä¶]\nWithout shared, conformed dimensions, a dimensional model becomes a standalone application. Isolated stovepipe data sets that cannot be tied together are the bane of the DW/BI movement as they perpetuate incompatible views of the enterprise. If you have any hope of building a robust and integrated DW/BI environment, you must commit to the enterprise bus architecture. When dimensional models have been designed with conformed dimensions, they can be readily combined and used together.\n\nConformed dimensions are a clear advantage of dimensional models and are essential to their success. However, the effort required to achieve this conformity is easy to underestimate. Agreeing across departments on consistant names, values and definitions for business concepts is challenging. This is especially true when systems have evolved organically in different parts of the organisation or come from multiple vendors."
  },
  {
    "objectID": "post/2026/data-warehouse-toolkit-definitive-guide-to-dimensional-modelling/index.html#reflections",
    "href": "post/2026/data-warehouse-toolkit-definitive-guide-to-dimensional-modelling/index.html#reflections",
    "title": "Reading: ‚ÄòThe Data Warehouse Toolkit: The Definitive Guide to Dimensional Modeling‚Äô by Ralph Kimball and Margy Ross",
    "section": "Reflections",
    "text": "Reflections\n\nData as a Product\nThe book does not describe it in these terms. However, it is clear that the authors advocate for the application of the product management method to the design of the data warehouse. They suggest a close relationship with the end users, understanding their needs and designing the dimensional model in accordance. They urge readers to consider the value to the business when making decisions. In many of their design guidelines, they emphasise ease of use for business teams. They recommend design solutions that engineers might view as suboptimal or overly complex in service of user experience. This is obviously how it should be.\nIt made me think of a principle that a former employer added to their group data strategy: ‚Äúdata as a product‚Äù. The goal was to change the company‚Äôs thinking on this subject. This principle especially targeted data-producing teams, encouraging them to treat data as part of their product surface area. One-sided schema changes, poor documentation, unreported incidents, data-quality all impacted downstream users (analysts, data scientists, consumers of internal reports). These problems slowed us down.\nI believe this was the right approach, and I was pleased to see the book echo this idea.\n\n\nVocabulary\nI had never encountered the terms used in this book to describe tables, techniques, columns, etc. I knew about fact tables and dimension tables. However, I had never heard of outrigger dimensions, didn‚Äôt know that fact tables had different names like accumulating snapshot fact table. When I saw the word grain, I immediately understood what it described, but I don‚Äôt think I ever heard this term. I definitely used the word level myself (at the user level, at the hit level) and I had discussions where granularity came up.\nIn a past project, without being aware of it at the time, I asked our central data engineering teams to improve an outrigger dimension for product certificates. I asked for the slow-changing dimension to go from Type 1 overwrite to Type 2 add new row. But we never discussed the change in those terms. We talked about ‚Äúmaking the historical relationship between products and certificates available for analysis‚Äù. We wanted to historicise the table. This word was difficult to pronounce correctly in meetings.\nWhy didn‚Äôt we use this vocabulary?\n\nCould these terms be confined to data engineering circles and left behind when other disciplines are involved?\nAre these words in books, but rarely found in the field?\nThis is a book written by authors from the USA and my work experience is in Germany and France with lots of native and non-native english speakers. Could it be the reason?\n\n\n\nUser experience improvements\nAs a long-time user, I was very happy to read the following advice. The points will improve your end users‚Äô life and save them from making mistakes:\n\nProvide human-understandable labels in addition to the technical codes and abbreviations. Quoting from the book:\n\nCryptic abbreviations, true/false flags, and operational indicators should be supplemented in dimension tables with full text words that have meaning when independently viewed. Operational codes with embedded meaning within the code value should be broken down with each part of the code expanded into its own separate descriptive dimension attribute.\n\nSimilarly, the book describes three different ways to handle Null values that make a ton of sense:\n\nIn dimension tables, the authors recommend substituting Unknown or Not Applicable in place of the null value. This applies when a dimension row is incomplete or when some attributes don‚Äôt apply to all rows.\nForeign key columns should never include nulls. Otherwise, referential integrity is broken i.e.¬†all fact rows should be associated with all the corresponding dimensions.\nIn fact tables, numeric measurements can be left null because calculations like sums and averages will correctly handle it. Replacing null with a 0 would bias the average, for example.\n\nProvide a Date dimension so that users can easily filter or group by date-related attributes. I like an easy way to access weekend vs.¬†weekdays , the full name for days (Monday, Tuesday, ‚Ä¶), months (January, February, ‚Ä¶) or even quarters (2026 Q1, 2026 Q2, ‚Ä¶) with a simple join. Even better if public holidays can be accessed or if dates important to the business can be retrieved. I have parsed and formatted dates countless times in SQL or Python, just to extract details like month names.\nFinally, in the last chapter of the book, the authors focus on the implications that big data has on data warehouses and ETL systems. While doing so, they write ‚ÄúAdd value to data as soon as possible‚Äù:\n\nYou should apply filtering, cleansing, pruning, conforming, matching, joining, and diagnosing at the earliest touch points possible. [‚Ä¶] Conforming takes the active step of placing highly administered enterprise attributes into major entities such as customer, product, and date. The existence of these conformed attributes allows high value joins to be made across separate application domains. A shorter name for this step is ‚Äúintegration!‚Äù Diagnosing allows many interesting attributes to be added to data, including special confidence tags and textual identifiers representing behavior clusters identified by a data mining professional.\n\nIn my experience, data science teams often extract data from the warehouse, create insights in silos, and rarely return those insights to the warehouse. As a result, teams often duplicate work in silos or get stuck because they cannot reproduce time-intensive research or advanced data science. I appreciate the idea of enriching data as early as possible, especially by integrating results from company-wide data mining activities."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi, I‚Äôm Gary!",
    "section": "",
    "text": "I‚Äôm a freelance product manager for data & AI products, and a lead product analyst.\nI mostly work with large international companies, which seek to accelerate their digital transformation. I help them build successful products and sophisticate their practices.\nYou can read more about me and the services I offer below.\nI am currently available for work and based in Berlin, Germany.\n\n\n\n\n\n\n\nC‚Äôest moi :)"
  },
  {
    "objectID": "index.html#what-i-can-do-for-you",
    "href": "index.html#what-i-can-do-for-you",
    "title": "Hi, I‚Äôm Gary!",
    "section": "What I can do for you",
    "text": "What I can do for you\n\nInterim leadership\nStep in temporarily (3‚Äì12 months) to accelerate your digital transformation or bridge gaps.\n\nProduct manager for your data & AI product(s)\nData leadership for your projects and teams\n\n\n\nProject-based collaboration\nJoin your team to deliver specific outcomes, from strategy to execution.\n\nBuild strong foundations in preparation for AI and ML use cases\nStructure your teams and data architecture to generate insights faster\nSophisticate the practice of analytics in your product organisation\nSupport with:\n\nExploratory data analysis and insight generation\nKPI definition and reporting frameworks\nA/B testing and experimentation programs\nETL, automation, and data pipeline work"
  },
  {
    "objectID": "index.html#advantages-of-working-with-me",
    "href": "index.html#advantages-of-working-with-me",
    "title": "Hi, I‚Äôm Gary!",
    "section": "Advantages of working with me",
    "text": "Advantages of working with me\n\n10+ years of experience in data and analytics, with a track record of delivering impact in large corporate environments. I‚Äôve worked with international companies in digital transformation like Henkel and Sodexo, and a pure-player scale-up like Zalando.\nProven ability to navigate complexity: I‚Äôve mitigated risks in high-stakes product launches (millions of euros at risk), worked on cross-market features with millions of users, uncovered insights from terabytes of user behavior data.\nCustomer-focused and methodical: I bring battled-tested approaches developed during high-growth years at Zalando. I contributed to Zalando‚Äôs group data strategy and produced several documents who made their way into the company‚Äôs product development best practices.\nHands-on leadership: I can lead teams and projects, but I‚Äôm also happy to roll up my sleeves and execute when needed. I keep my coding skills fresh (SQL, Python, R) so I can automate and analyse on my own.\nSolo freelancer: more affordable than consulting firms and the person you meet is the person you get."
  },
  {
    "objectID": "index.html#what-companies-say-about-me",
    "href": "index.html#what-companies-say-about-me",
    "title": "Hi, I‚Äôm Gary!",
    "section": "What companies say about me",
    "text": "What companies say about me\n\n\n\n  \n    \n        \n            \n                \n            \n\n            \n                Kevin Albrand\n                Head of data - Excellence op√©rationnelle\n            \n            \n            \n            \n        \n        \n            \n                I worked with Gary on an data and analytics product, aiming at helping users to take better decisions to manage the profitability. As a product manager, Gary created a very valuable relationship both with the team and his business sponsors, listening to the needs and ensuring that the product was driven to provide as much value as possible to the organization. His dual skills technical / product helped him to navigate between user discovery, hypothesis checks, and impact driving. I would recommended Gary for any data analytics oriented products.\n            \n        \n    \n  \n    \n        \n            \n                \n            \n\n            \n                Dr. Arman Nassirtoussi\n                Head of Product (AI, Data, Analytics | Personalisation)\n            \n            \n            \n            \n        \n        \n            \n                Gary is super structured and diligent, he deep-dived in many new topics and worked on shaping a cohesive strategy for the business cluster, we were shaping analytics standards and ways-of-working for. He is very collaborative and pursues what he has at hand carefully. He has been in the space of data driven product development for many years now and can definitely bring a strong perspective and a lot of experience to his future projects, and accelerate success.\n            \n        \n    \n  \n    \n        \n            \n                \n            \n\n            \n                Rob Manzano\n                Director, Product Insights\n            \n            \n            \n            \n        \n        \n            \n                Gary rapidly proved himself a trusted ally on whom I could depend. His resolve was unyielding throughout, and I can testify to his composed, pragmatic approach and resilience.\n            \n                Possessing thorough knowledge in his field, Gary also comprehends its strategic role within the broader organisation. His ability to express himself, articulate his ideas clearly, and propel the team with his solutions was highly commendable.\n            \n                Gary is someone who persistently sets the standard, displays empathy, and shows respect. He welcomes feedback, self-reflects, and evolves.\n            \n                I unreservedly endorse Gary, confident that he will continue to excel in his career. I recommend him wholeheartedly for any opportunities that come his way.\n            \n        \n    \n\n\n\nNo matching items\n\nMore references available on LinkedIn."
  },
  {
    "objectID": "index.html#want-to-work-with-me",
    "href": "index.html#want-to-work-with-me",
    "title": "Hi, I‚Äôm Gary!",
    "section": "Want to work with me?",
    "text": "Want to work with me?\nEmail me\nLinkedIn and Mastodon work too."
  },
  {
    "objectID": "post/2025/fridge-sound-analysis/index.html",
    "href": "post/2025/fridge-sound-analysis/index.html",
    "title": "Analysing the sound of my fridge",
    "section": "",
    "text": "This is a fun little project that I wanted to try for a bit now. You see, my fridge at home is very old. It has always been noisy, but I‚Äôve been able to ignore it for many years.\nHowever, lately, something changed. It feels like the compressor runs louder and more frequently than before (too high of a frequency according to my nerves).\nI wanted to quantify two things:\n\nHow long the compressor runs for each time,\nHow long between each compressor run.\n\nI tried timing it manually, but I ran into issues and I didn‚Äôt trust my measurements to be accurate. It‚Äôs tedious to stay alert to a fridge‚Äôs background hum while at home. üòÉ\nI wanted something more scientific and decided to use my iPad and the Voice Memos app to record the sound. I had the vague idea that with the sound waves and timestamps I could reach my goals.\nTo make this easy, I wanted to record for rather long periods of time and to have clean recordings. So I left the iPad record while I was away for most of the day.\nMy first idea was to do my analysis with the naked eye, but later I decided that it would be more fun to find a programmatic way to do it. I wanted to put my recently-acquired Mistral AI subscription to the test."
  },
  {
    "objectID": "post/2025/fridge-sound-analysis/index.html#backstory",
    "href": "post/2025/fridge-sound-analysis/index.html#backstory",
    "title": "Analysing the sound of my fridge",
    "section": "",
    "text": "This is a fun little project that I wanted to try for a bit now. You see, my fridge at home is very old. It has always been noisy, but I‚Äôve been able to ignore it for many years.\nHowever, lately, something changed. It feels like the compressor runs louder and more frequently than before (too high of a frequency according to my nerves).\nI wanted to quantify two things:\n\nHow long the compressor runs for each time,\nHow long between each compressor run.\n\nI tried timing it manually, but I ran into issues and I didn‚Äôt trust my measurements to be accurate. It‚Äôs tedious to stay alert to a fridge‚Äôs background hum while at home. üòÉ\nI wanted something more scientific and decided to use my iPad and the Voice Memos app to record the sound. I had the vague idea that with the sound waves and timestamps I could reach my goals.\nTo make this easy, I wanted to record for rather long periods of time and to have clean recordings. So I left the iPad record while I was away for most of the day.\nMy first idea was to do my analysis with the naked eye, but later I decided that it would be more fun to find a programmatic way to do it. I wanted to put my recently-acquired Mistral AI subscription to the test."
  },
  {
    "objectID": "post/2025/fridge-sound-analysis/index.html#mistral-ai",
    "href": "post/2025/fridge-sound-analysis/index.html#mistral-ai",
    "title": "Analysing the sound of my fridge",
    "section": "Mistral AI",
    "text": "Mistral AI\nThe company and its models are often mentioned as European alternatives to US firms and their models. I wanted to give it a go and get familiar with it. In addition, Mistral and I are both born in France. These were enough reasons for me to sign up to the Pro tier and to start experimenting.\nMy setup was:\n\nLocal machine (Macbook Air M3)\nWorking in Python inside Visual Studio Code\nUsing Codestral within VS Code itself 1\n\nCodestral is Mistral AI‚Äôs generative AI model explicitly designed for code generation tasks.\nI went into this with prior Python coding knowledge, but I had never done any audio processing with it. So I gave the LLM a broad description of what I wanted to achieve. I didn‚Äôt recommend packages, or specific methods. I only described what I had and what I wanted to achieve very broadly.\nInitial prompt:\n\nI have a file ‚Äú.m4a‚Äù which is a recording of my fridge at home. I want to analyse the sound inside of this file to identify each time the fridge‚Äôs compressor runs. The sound is very distinctive and I know that in this file there are two compressor runs which lasts very long times. What should I do in Python?\n\nI‚Äôll cut the story short, it disappointed me for the task at hand:\n\nI spent a few hours re-prompting, giving more precise instructions, explicitly asking for things and I never managed to get to something useful. The script was producing code that worked. All the packages used were safe and well-known. It was ‚Äúanalysing‚Äù my audio file and producing good data visualisations, but it couldn‚Äôt identify the compressor runs properly.\nThe code produced was very slow to run. It took between 5 and 10 mins each time. This made debugging and iterating very slow.\nFinally, in a different session, I tried prompting Mistral on the web. I supposed that my configuration in VS Code might be faulty. At first, it gave me interesting results, but I still stumbled in the last mile and couldn‚Äôt get to something useful.\n\n\n\n\n\nLast analysis generated before I gave up. The dashed red line is supposed to be a threshold used for detection. However, it does not seem to matter since data below the threshold is highlighted as compressor run‚Ä¶"
  },
  {
    "objectID": "post/2025/fridge-sound-analysis/index.html#claude-to-the-rescue",
    "href": "post/2025/fridge-sound-analysis/index.html#claude-to-the-rescue",
    "title": "Analysing the sound of my fridge",
    "section": "Claude to the rescue!",
    "text": "Claude to the rescue!\nFrustrated, I decided to copy and paste my initial prompt in Claude‚Äôs web UI. I am not a paying subscriber so I am using Claude Sonnet 4.5 on the free tier.\nAnd‚Ä¶ To my surprise, it worked in 7 iterations. ü•≤\n\nThe very first script that it generated was much faster than the one from Codestral: less than ten seconds vs.¬†several minutes before.\nIts first attempt produced a graph that didn‚Äôt show which sections of the audio were identified as compressor runs. After I asked for it, it produced code that generated this graph. It was very easy to go from there to a finished analysis.\n\n\n\nIt produced parameters that I could manually tune. It explained to me what these parameters were doing with example values to illustrate. For example:\n\nLower (e.g., 60): More sensitive, catches quieter compressor runs but may have false positives\nHigher (e.g., 80-90): Only catches louder/clearer runs, fewer false positives\n\nAfter my 7th iteration, I had this output (pretty clean!):\n\n\nTotal segments above threshold: 11\nSegments after filtering (‚â•900s): 2\nDetected 2 compressor run(s):\n\nRun 1: 18.07min - 49.33min (duration: 31.27min)\nRun 2: 132.93min - 164.40min (duration: 31.47min)\n\nTime between runs:\n\nBetween Run 1 and Run 2: 83.60min\n\n\n\nThe code generated by Claude was better because:\n\nThe detection method worked much better from the get-go. And the code generated ran fast from the start too.\nIt used parameters whose values were based on my audio file. For example, the detection threshold was the nth percentile of a metric computed on the audio data itself. Manually tuning this percentile threshold was intuitive too.\nIt even proactively handled an edge case: what if the recording stops before the end of a compressor run.\nI have the feeling that Claude was more responsive to my input and request for change too. Maybe this has to do with a difference in the length of the context windows (6 times longer for Claude Sonnet 4.5 vs.¬†Codestral in my use case)2.\n\nIn any case, this provided me with good findings and lessons learned for future projects, especially if I want to persevere with Mistral."
  },
  {
    "objectID": "post/2025/fridge-sound-analysis/index.html#back-to-the-analysis",
    "href": "post/2025/fridge-sound-analysis/index.html#back-to-the-analysis",
    "title": "Analysing the sound of my fridge",
    "section": "Back to the analysis",
    "text": "Back to the analysis\nI pointed the final script (code generated by Claude with my manual parameter-tuning) to a recording that lasts eight hours and thirty minutes:\n\n\nCode\nimport librosa\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# 1. Load the audio file\naudio, sr = librosa.load(\"audio/fridge-1-clean.m4a\", sr=None)\n\n# 2. Calculate RMS energy over time windows\nframe_length = sr * 2  # Adjust this\nhop_length = sr * 4  # Adjust this\nrms = librosa.feature.rms(y=audio, frame_length=frame_length, hop_length=hop_length)[0]\n\n# 3. Detect high-energy periods (compressor running)\nthreshold = np.percentile(rms, 65)  # Adjust this\nis_running = rms &gt; threshold\n\n# 4. Get time values\ntimes = librosa.frames_to_time(np.arange(len(rms)), sr=sr, hop_length=hop_length)\n\n# 5. Extract running segments\nrunning_segments = []\nstart = None\nfor i, running in enumerate(is_running):\n    if running and start is None:\n        start = i\n    elif not running and start is not None:\n        running_segments.append((times[start], times[i - 1]))\n        start = None\nif start is not None:  # Handle case where recording ends while running\n    running_segments.append((times[start], times[-1]))\n\n# 6. FILTER BY MINIMUM DURATION\nmin_duration = 900  # seconds - adjust this based on your compressor\nrunning_segments_filtered = [\n    (start, end) for start, end in running_segments if (end - start) &gt;= min_duration\n]\n\n# 7. Create mask for filtered segments only\nis_running_filtered = np.zeros_like(is_running, dtype=bool)\nfor start_time, end_time in running_segments_filtered:\n    mask = (times &gt;= start_time) & (times &lt;= end_time)\n    is_running_filtered |= mask\n\n# 8. Visualize with highlighted compressor runs\nplt.figure(figsize=(9, 3))\n\n# Plot the RMS energy in minutes\ntimes_minutes = times / 60\nplt.plot(times_minutes, rms, color=\"blue\", linewidth=1, label=\"RMS Energy\")\n\n# Highlight filtered compressor running periods\nplt.fill_between(\n    times_minutes,\n    0,\n    rms,\n    where=is_running_filtered,\n    color=\"red\",\n    alpha=0.3,\n    label=\"Compressor Running\",\n)\n\nplt.axhline(threshold, color=\"orange\", linestyle=\"--\", linewidth=1, label=\"Threshold\")\nplt.xlabel(\"Time (minutes)\")\nplt.ylabel(\"RMS Energy\")\nplt.title(f\"Fridge Compressor Activity Detection (min duration: {min_duration}s)\")\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# 9. Print detection summary\nprint(f\"\\nTotal segments above threshold: {len(running_segments)}\")\nprint(f\"Segments after filtering (‚â•{min_duration}s): {len(running_segments_filtered)}\")\nprint(f\"\\nDetected {len(running_segments_filtered)} compressor run(s):\")\nfor i, (start_time, end_time) in enumerate(running_segments_filtered, 1):\n    duration_minutes = (end_time - start_time) / 60\n    start_minutes = start_time / 60\n    end_minutes = end_time / 60\n    print(\n        f\"  Run {i}: {start_minutes:.2f}min - {end_minutes:.2f}min (duration: {duration_minutes:.2f}min)\"\n    )\n\n# 10. Calculate and print time between runs\nif len(running_segments_filtered) &gt; 1:\n    print(f\"\\nTime between runs:\")\n    for i in range(len(running_segments_filtered) - 1):\n        end_of_current = running_segments_filtered[i][1]\n        start_of_next = running_segments_filtered[i + 1][0]\n        gap_minutes = (start_of_next - end_of_current) / 60\n        print(f\"  Between Run {i + 1} and Run {i + 2}: {gap_minutes:.2f}min\")\n\n\n\n\n\n\n\n\n\n\nTotal segments above threshold: 267\nSegments after filtering (‚â•900s): 4\n\nDetected 4 compressor run(s):\n  Run 1: 67.60min - 103.00min (duration: 35.40min)\n  Run 2: 186.40min - 222.13min (duration: 35.73min)\n  Run 3: 307.53min - 341.60min (duration: 34.07min)\n  Run 4: 427.20min - 460.67min (duration: 33.47min)\n\nTime between runs:\n  Between Run 1 and Run 2: 83.40min\n  Between Run 2 and Run 3: 85.40min\n  Between Run 3 and Run 4: 85.60min\n\n\nThe compressor runs are easily identifiable in the plot and, as the red zones show, the detection mechanism works very well at identifying them."
  },
  {
    "objectID": "post/2025/fridge-sound-analysis/index.html#conclusion",
    "href": "post/2025/fridge-sound-analysis/index.html#conclusion",
    "title": "Analysing the sound of my fridge",
    "section": "Conclusion",
    "text": "Conclusion\nWhat we were all dying to know is now revealed to us. The compressor in my fridge runs for roughly 33-35 minutes each time and it runs every 83-85 minutes."
  },
  {
    "objectID": "post/2025/fridge-sound-analysis/index.html#footnotes",
    "href": "post/2025/fridge-sound-analysis/index.html#footnotes",
    "title": "Analysing the sound of my fridge",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is not natively supported and needs to be configured via Continue.dev‚Äôs extension (documentation).‚Ü©Ô∏é\nMistral‚Äôs documentation mentions 32k for Codestral. And Anthropic gives an example where 200k tokens are the size of the standard window, which would correspond to my free tier.‚Ü©Ô∏é"
  },
  {
    "objectID": "post/2025/posit-conf-2025-my-selection-of-youtube-talks/index.html",
    "href": "post/2025/posit-conf-2025-my-selection-of-youtube-talks/index.html",
    "title": "My selection of posit::conf(2025) talks to watch on YouTube",
    "section": "",
    "text": "Posit is the company behind RStudio and many widely-used R packages (such as ggplot2, the tidyverse). They also develop tools for other languages like Python (including their new data science IDE called Positron, and plotnine, their port of ggplot2 to Python). In short, they create a lot of useful tools for the data science community.\nEvery year, they organise a conference, and they just released the recordings of this year‚Äôs talks on YouTube.\nHere are my recommended talks (in no particular order):\n\nHow I got unstuck with Python (Julia Silge, Posit)\nAs I mentioned in a previous post, I started learning Python after R and found some key differences between the two languages challenging. Finding the right tools was also challenging. Julia Silge highlights pyenv as a turning point for her and it was for me too.\nOn macOS, Python isn‚Äôt pre-installed. Instead, it comes with the Command Line Tools, which must be installed separately. This made me feel uncomfortable. I didn‚Äôt like messing with the system‚Äôs python version (installing and uninstalling libraries). Also, the version was quite old (2.x) when I set up my local environment. Then I discovered pyenv and the concept made sense : it‚Äôs possible to install separate Python versions and to create isolated virtual environments. That‚Äôs what I chose, and still use today. Though maybe not much longer.\nJulia Silge has moved to using uv. I have read so many good things about it. It is next on my list.\nFrom messy to meaningful data: LLM-powered classification in R (Dylan Pieper)\nThe talk covers more, but it served as a good introduction to the ellmer package for me: how to interact with LLMs and get structured output in R. I also enjoyed the rest of the talk, especially the speaker‚Äôs use of traditional ML models as benchmark against the LLM.\nPosit and the modern data stack: Using Posit, Snowflake, and Databricks (James Blair)\nHere, the title is a bit misleading. The talk focuses on something more exciting: a Positron feature called Positron Assistant, which provides LLM-assisted coding and data analysis.\nSo far, these assistants required data to be available locally, which is a very unlikely scenario when working with enterprise data sources (data volumes are too large, downloading data is a privacy and security no-go).\nIn the demo, James Blair shows how the assistant can work with the data available in Databricks or Snowflake. You need to set the connection up first and then it will work off of the table names, table schemas and a small sample of data. It then produces code that will run on the Databricks cluster for example.\nThis feels like a nice step forward. It looks like the integration required a collaboration between Posit, Databricks and Snowflake. I‚Äôm still unclear about what enterprises need to do to implement this, and what metadata must be provided if any. Nevertheless, it‚Äôs an interesting development. Of course, the performance of these LLMs in this scenario remains to be seen.\nExpanding Quarto‚Äôs Capabilities with Lua (Christophe Dervieux, Posit)\nI built my website using Quarto. During the process, I encountered limitations and came across mentions of Lua filters and Quarto extensions. I decided to explore these more advanced topics later. My main goal was simply to get the site online.\nChristophe Dervieux‚Äôs talk was a great introduction for me. While I‚Äôm not yet confident enough to develop with Lua filters, I now understand their role. It was helpful to see the pipeline that transforms .qmd documents into HTML, and where Lua filters come into play."
  },
  {
    "objectID": "link/2025/11/19/python-not-great-package-for-data-science/index.html",
    "href": "link/2025/11/19/python-not-great-package-for-data-science/index.html",
    "title": "‚ÄòPython is not a great language for data science‚Äô",
    "section": "",
    "text": "Here, I link to the second post from Claus Wilke in his series ‚ÄúPython is not a great language for data science.‚Äù\nI have read various posts about the differences between R and Python over the years and the details are usually too obscure for me. The inner workings of these languages are dissected in a way that I cannot relate to.\nIn this case, I did relate to it. I can recommend the article to anyone in data science who has to evolve between R and Python. Claus described issues I‚Äôve encountered without fully understanding them.\nThe points that caught my attention are:\n\nCall-by-reference semantics: the fact that calling a function on a data object will modify the object passed as argument, even when no assignment to a new variable is done.\nLack of built-in missing values: different data analysis libraries treat missing values differently and make assumptions which are not obvious\nLack of non-standard evaluation: mutate() and arrange()in the tidyverse make common data science tasks so much easier to do and read. The syntax is cleaner too.\n\nFinally, don‚Äôt get me wrong, I am not ditching Python at all. I enjoy using it. It‚Äôs good to put words on frustrations and misunderstandings I‚Äôve had in the past.\n\nIn addition, in Claus‚Äô first post from the series, another topic hit close to home:\n\nSo here is a typical experience I commonly have with students who use Python. A student comes to my office and shows me some result. I say ‚ÄúThis is great, but could you quickly plot the data in this other way?‚Äù or ‚ÄúCould you quickly calculate this quantity I just made up and let me know what it looks like when you plot it?‚Äù or similar. Usually, the request I make is for something that I know I could do in R in just a few minutes. Examples include converting boxplots into violins or vice versa, turning a line plot into a heatmap, plotting a density estimate instead of a histogram, performing a computation on ranked data values instead of raw data values, and so on. Without fail, from the students that use Python, the response is: ‚ÄúThis will take me a bit. Let me sit down at my desk and figure it out and then I‚Äôll be back.‚Äù Now let me be absolutely clear: These are strong students. The issue is not that my students don‚Äôt know their tools. It very much seems to me to be a problem of the tools themselves. They appear to be sufficiently cumbersome or confusing that requests that I think should be trivial frequently are not.\n\nThis happened to me too. At first, I thought the problem was coming from me. I am used to R‚Äôs ggplot2 library for data visualisation and I felt really slow in Python.\nIn a footnote, Claus Wilke notes that students who use plotnine do not have this problem. This matches with my own experience. Plotnine is a port of ggplot2 in Python. And it is now my first choice.\nAt first, I tried to learn matplotlib and I quickly realised that this was a no-go for me (simple plots were hard to build, commands hard to remember for me). Then, I saw many people recommend seaborn, which is based on matplotlib. It is better and more intuitive for me, but I still find working with it to be hard. I struggle especially when it comes to facets.\nLet‚Äôs say you plot aggregated data over several countries. And you want to see if there are differences between countries. In ggplot2, you don‚Äôt need to change much in your current code. One line of code allows you to facet by the dimension country and it‚Äôs easy to facet by two dimensions if needed.\nIn seaborn, some plotting functions support something similar, but not all of them do, which complicates things. Otherwise, you need to create a FacetGrid object, which is a slightly different way of building a plot. And then, on top of that, adding a plot title is done differently when the plot has facets.\nAll of this requires to memorise different commands for different use cases. And it‚Äôs a big slow down for me. So I relate to the example given above and I would also recommend plotnine. My only gripe is that is updated on a different schedule than R‚Äôs ggplot2 and both can be sometimes out of sync. It only happened to me once though that something that I knew was possible in ggplot2 was not in plotnine."
  }
]